# Configuration for data constrained scaling law discovery with OpenEvolve
max_iterations: 100
checkpoint_interval: 1
log_level: "INFO"
random_seed: 123

# LLM configuration
llm:
  primary_model: gemini-2.5-pro
  primary_model_weight: 1.0
  secondary_model: gpt-5
  secondary_model_weight: 0.0
  api_base: "http://api.llm.wq/v1"
  max_tokens: 16384
  timeout: 240
  retries: 10
  retry_delay: 20

# Prompt configuration
prompt:
  system_message: |
    You are an expert in scaling laws and machine learning who specializes in discovering and improving scaling law functions for different LLM training scenarios, with a focus on mixed-effects modeling.
    Your task is to evolve both the `scaling_law_func` function (which will represent the fixed-effect part of the model) and the `fit_scaling_law` optimization algorithm (which must fit a full mixed-effects model) to better model the relationship between training data characteristics and model loss (negative logprobs). The model must account for variance across different tasks (e.g., different MMLU problems).

    **IMPORTANT: IMPORTANT: The fixed-effect scaling law function (`scaling_law_func`) must use no more than 6 parameters (e.g., A, B, \alpha, \beta, L_\infty).** The `fit_scaling_law` function will need to fit these 6 parameters plus all the per-problem random-effect parameters, but it should **only return the 6 fixed-effect parameters.**

    Focus on mathematical accuracy, cross-dataset generalization, parameter efficiency (for the fixed effects), numerical/theoretical stability, and robust estimation of fixed effects despite high variance from random effects.

    **DATA CHARACTERISTICS:**
    - Features: [problem_id, params, tokens] - (N, 3) input array.
    - problem_id: A categorical identifier for each unique MMLU problem.
    - params (P): Model parameter count (e.g., 1.4e7 to 7e10).
    - tokens (D): Model token count (e.g., 1.8e11 or 3.6e13).
    - Labels: loss - scalar output (negative log of the probability, e.g., 0.0001 to 10.2500).
    - Dataset size: Large. Consists of K distinct problem_ids and 66 models, for a total of N = 66 K data points. (K=8425 for train and K=2808 for validation).
    - Goal: Model loss as a function of fixed effects [params, tokens] and a random effect (e.g., a random intercept) based on [problem_id].

    The function signatures must remain:

    ```python
    def scaling_law_func(data_points, hyperparams):
        # data_points: (N,3) array with columns [problem_id, params, tokens]
        # params: Array of parameter counts
        # tokens: Array of token counts
        # hyperparams: Array of up to 6 FIXED-EFFECT parameters (the global scaling law)
        # Returns: Predicted loss contribution from the FIXED-EFFECTS ONLY.
        # (e.g., returns A/params**alpha + B/tokens**beta + L_inf)

    def fit_scaling_law(data_points, loss_values):
        # data_points: (N,3) array with columns [problem_id, params, tokens]
        # loss_values: Array of corresponding loss values
        # Returns: Optimized FIXED-EFFECT hyperparameters (up to 6 parameters)
        #
        # NOTE: This function must internally model and fit the random effects
        # (e.g., a per-problem_id intercept) to properly isolate and find
        # the fixed-effect parameters. It may use libraries like statsmodels
        # or implement a joint optimization, but its final output must be
        # *only* the global, fixed-effect hyperparameters.
    ```

    Write all improvements between # EVOLVE-BLOCK-START and # EVOLVE-BLOCK-END markers.

    You are not allowed to use input-dependent feature in scaling_law_func, e.g., median / min / max / etc.

  num_top_programs: 3
  num_diverse_programs: 2
  use_template_stochasticity: true

# Database configuration for evolution
database:
  population_size: 100
  archive_size: 50
  num_islands: 5
  migration_interval: 25
  migration_rate: 0.1
  elite_selection_ratio: 0.1
  exploration_ratio: 0.2
  exploitation_ratio: 0.7
  feature_dimensions: ["combined_score", "complexity", "diversity"]
  feature_bins: 10

# Evaluator configuration
evaluator:
  timeout: 1200
  max_retries: 3
  cascade_evaluation: false
  cascade_thresholds: [0.3, 0.6]
  parallel_evaluations: 4
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: false
max_code_length: 100000